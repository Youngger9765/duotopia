from sqlalchemy import create_engine
from sqlalchemy.orm import declarative_base, sessionmaker
import os
from dotenv import load_dotenv

load_dotenv()

DATABASE_URL = os.getenv(
    "DATABASE_URL", "postgresql://duotopia_user:duotopia_pass@localhost:5432/duotopia"
)

# ğŸ”§ å»¶é²è¼‰å…¥ï¼šåªåœ¨å¯¦éš›ä½¿ç”¨æ™‚æ‰å»ºç«‹è³‡æ–™åº«é€£ç·š
# é€™æ¨£ conftest.py å¯ä»¥åœ¨æ²’æœ‰ DATABASE_URL çš„æƒ…æ³ä¸‹è¢«è¼‰å…¥ï¼ˆå–®å…ƒæ¸¬è©¦ï¼‰
_engine = None
_SessionLocal = None


def get_engine():
    """å»¶é²å»ºç«‹è³‡æ–™åº«å¼•æ“

    Connection pool configuration (2024-12-10):
    - FIX #5: Reduced to safer values for Supabase Free Tier (~25 connection limit)
    - pool_size=10: Base pool always kept alive (was 15)
    - max_overflow=10: Additional connections created on demand (was 15)
    - Total: 20 connections per instance (safer for Supabase Free Tier)
    - pool_timeout=10: Faster failure feedback (down from default 30s)
    - pool_recycle=3600: Recycle connections every hour to avoid idle timeouts
    - pool_pre_ping=True: Health check before using connection

    This fixes production issue where 20 concurrent audio uploads exhausted
    the previous 15-connection limit, causing timeout errors.

    NOTE: If running multiple backend instances, total connections = pool_size Ã— instances
    """
    global _engine
    if _engine is None:
        # FIX #5: Safer configuration for Supabase Free Tier
        # Read pool configuration from environment variables with safe defaults
        pool_size = int(os.getenv("DB_POOL_SIZE", "10"))  # Was 15
        max_overflow = int(os.getenv("DB_MAX_OVERFLOW", "10"))  # Was 15
        pool_timeout = int(os.getenv("DB_POOL_TIMEOUT", "10"))

        # SQLite vs PostgreSQL specific configurations
        connect_args = {}
        if DATABASE_URL.startswith("postgresql"):
            connect_args = {
                "connect_timeout": 10,  # é€£ç·šè¶…æ™‚ 10 ç§’
                "options": "-c statement_timeout=30000",  # SQL åŸ·è¡Œè¶…æ™‚ 30 ç§’
            }
        elif DATABASE_URL.startswith("sqlite"):
            connect_args = {
                "check_same_thread": False  # SQLite: Allow multiple threads
            }

        _engine = create_engine(
            DATABASE_URL,
            pool_pre_ping=True,  # æ¯æ¬¡å–å¾—é€£ç·šå‰å…ˆæ¸¬è©¦ï¼Œé˜²æ­¢ä½¿ç”¨æ–·ç·šçš„é€£ç·š
            pool_recycle=3600,  # 1å°æ™‚å›æ”¶é€£ç·šï¼Œé¿å…é•·æ™‚é–“é–’ç½®è¢«é—œé–‰
            pool_size=pool_size,  # é€£ç·šæ± å¤§å° (default: 10)
            max_overflow=max_overflow,  # æœ€å¤§æº¢å‡ºé€£ç·šæ•¸ (default: 10)
            pool_timeout=pool_timeout,  # é€£ç·šç­‰å¾…è¶…æ™‚ (é™ä½: 30s â†’ 10s å¿«é€Ÿå¤±æ•—)
            connect_args=connect_args,
        )
    return _engine


def get_session_local():
    """å»¶é²å»ºç«‹ Session maker"""
    global _SessionLocal
    if _SessionLocal is None:
        _SessionLocal = sessionmaker(
            autocommit=False, autoflush=False, bind=get_engine()
        )
    return _SessionLocal


# Backward compatibility for code expecting SessionLocal symbol
SessionLocal = get_session_local()

Base = declarative_base()


def get_db():
    """å–å¾—è³‡æ–™åº« session"""
    SessionLocal = get_session_local()
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


def init_db():
    """åˆå§‹åŒ–è³‡æ–™åº« - æ‡‰è©²ä½¿ç”¨ alembic ç®¡ç† schema"""
    import models  # noqa: F401 - Import models to register them

    # ğŸš¨ ä¸å†ç›´æ¥ä½¿ç”¨ create_allï¼Œæ”¹ç”¨ alembic ç®¡ç†
    # Base.metadata.create_all(bind=engine)

    print("âš ï¸  è«‹ä½¿ç”¨ alembic upgrade head ä¾†å»ºç«‹è³‡æ–™è¡¨")
    print("   ä¸è¦ç›´æ¥ä½¿ç”¨ init_db() ç¹é alembic ç®¡ç†")
