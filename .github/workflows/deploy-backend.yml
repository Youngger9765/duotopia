name: Deploy Backend (Cloud Run)

# üéØ UPDATED STRATEGY: Deploy to Cloud Run for ALL environments
# üìù Main branch ‚Üí Production Cloud Run (primary deployment)
# üìù Staging/other branches ‚Üí Staging Cloud Run
# üìù VM deployment is deprecated and will be phased out

on:
  workflow_dispatch:  # ÊâãÂãïËß∏Áôº
  push:
    branches:
      - main
      - staging
      - develop
      # Note: main branch uses VM deployment (deploy-vm-prod.yml)
    paths:
      # ÂæåÁ´ØÁõ∏ÈóúÊ™îÊ°à
      - 'backend/**'
      - '!backend/**/*.md'
      - '!backend/tests/**'
      - 'requirements*.txt'
      - 'alembic.ini'
      - 'Dockerfile'
      - '.github/workflows/deploy-backend.yml'
  pull_request:
    branches:
      - main
      - staging
      - develop
    paths:
      - 'backend/**'
      - '!backend/**/*.md'
      - 'requirements*.txt'
      - 'alembic.ini'
      - 'Dockerfile'
      - '.github/workflows/deploy-backend.yml'

env:
  PROJECT_ID: duotopia-472708
  REGION: asia-east1
  REPOSITORY: duotopia-repo

jobs:
  test-backend:
    name: üß™ Test Backend
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: migration_check
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    steps:
    - uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install backend dependencies
      working-directory: ./backend
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run Black formatting check
      working-directory: ./backend
      run: |
        echo "Checking Python code formatting with Black..."
        black --check . || (echo "‚ùå Black formatting check failed. Run 'black backend/' locally to fix." && exit 1)

    - name: Run Flake8 linting
      working-directory: ./backend
      run: |
        echo "Running Flake8 linting..."
        flake8 . --config=../.flake8

    - name: Apply migrations to fresh DB
      env:
        DATABASE_URL: postgresql://test:test@localhost:5432/migration_check
      working-directory: ./backend
      run: |
        echo "üîÑ Applying all migrations to fresh local PostgreSQL..."
        alembic upgrade head
        echo "‚úÖ All migrations applied successfully"

    - name: Check for model-migration sync
      # TODO: Remove continue-on-error after cleaning up legacy model-migration
      # differences (Issue #314). Currently there are pre-existing mismatches
      # (removed tables, type diffs, index diffs) that cause alembic check to fail.
      continue-on-error: true
      env:
        DATABASE_URL: postgresql://test:test@localhost:5432/migration_check
      working-directory: ./backend
      run: |
        echo "üîç Checking for uncommitted model changes..."
        if ! alembic check; then
          echo ""
          echo "‚ö†Ô∏è Model-migration differences detected (may include legacy diffs)"
          echo "üìù If you modified models, ensure you created a migration:"
          echo "üëâ cd backend && alembic revision --autogenerate -m 'description'"
          exit 1
        fi
        echo "‚úÖ Models and migrations are in sync"

    - name: Run unit tests
      # TODO: Remove continue-on-error after fixing unit tests (Issue #314)
      # Many tests/unit/ files depend on DB/external services and are not true unit tests
      continue-on-error: true
      working-directory: ./backend
      run: |
        echo "üß™ Running unit tests (no external dependencies)..."
        pytest tests/unit/ -n auto -v --tb=short

    - name: Run integration tests
      # TODO: Remove continue-on-error after fixing tests (Issue #314)
      # conftest.py hardcodes SQLite ‚Äî parallel workers cause table creation races
      continue-on-error: true
      working-directory: ./backend
      env:
        DATABASE_URL: ${{ secrets.STAGING_DATABASE_POOLER_URL }}
        TEST_DATABASE_URL: ${{ secrets.STAGING_DATABASE_POOLER_URL }}
      run: |
        echo "üß™ Running integration tests with staging database (pooler URL)..."
        pytest tests/ --ignore=tests/unit/ --ignore=tests/e2e/ --ignore=tests/benchmarks/ --ignore=tests/load_testing/ --ignore=tests/stress/ -v --tb=short

    - name: Test backend import
      working-directory: ./backend
      env:
        DATABASE_URL: ${{ secrets.STAGING_DATABASE_POOLER_URL }}
      run: python -c "import main; print('Backend imports successfully')"

  deploy-backend:
    name: üöÄ Deploy Backend
    needs: test-backend
    if: github.event_name != 'pull_request'
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set Environment Variables
      id: env_vars
      run: |
        # Deploy to Production for main branch, Staging for others
        if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
          echo "ENV_NAME=production" >> $GITHUB_OUTPUT
          echo "BACKEND_SERVICE=${{ secrets.PRODUCTION_BACKEND_SERVICE }}" >> $GITHUB_OUTPUT
          echo "üöÄ Deploying to PRODUCTION (Cloud Run)"
        elif [[ "${{ github.ref }}" == "refs/heads/develop" ]]; then
          echo "ENV_NAME=develop" >> $GITHUB_OUTPUT
          echo "BACKEND_SERVICE=${{ secrets.DEVELOP_BACKEND_SERVICE }}" >> $GITHUB_OUTPUT
          echo "üîß Deploying to DEVELOP"
        else
          echo "ENV_NAME=staging" >> $GITHUB_OUTPUT
          echo "BACKEND_SERVICE=${{ secrets.STAGING_BACKEND_SERVICE }}" >> $GITHUB_OUTPUT
          echo "üß™ Deploying to STAGING (Cloud Run)"
        fi

    - name: Authenticate to Google Cloud
      uses: google-github-actions/auth@v2
      with:
        credentials_json: ${{ secrets.GCP_SA_KEY }}

    - name: Set up Cloud SDK
      uses: google-github-actions/setup-gcloud@v2

    # Cloud Run ÊúâÈ†êË®≠ÁöÑ service accountÔºå‰∏çÈúÄË¶ÅÈ°çÂ§ñÁöÑ key file
    # GCS ÊúÉËá™Âãï‰ΩøÁî® Cloud Run ÁöÑË™çË≠â

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Configure Docker for Artifact Registry
      run: |
        gcloud auth configure-docker $REGION-docker.pkg.dev

    - name: Cache Docker layers
      uses: actions/cache@v3
      with:
        path: /tmp/.buildx-cache
        key: ${{ runner.os }}-buildx-${{ hashFiles('backend/requirements.txt', 'backend/Dockerfile') }}
        restore-keys: |
          ${{ runner.os }}-buildx-

    - name: Set Database Environment Variables
      id: db_env
      run: |
        # Use production, develop, or staging database based on branch
        if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
          echo "DATABASE_URL=${{ secrets.PRODUCTION_DATABASE_URL }}" >> $GITHUB_OUTPUT
          echo "ALEMBIC_DATABASE_URL=${{ secrets.PRODUCTION_DATABASE_POOLER_URL }}" >> $GITHUB_OUTPUT
          echo "SUPABASE_URL=${{ secrets.PRODUCTION_SUPABASE_URL }}" >> $GITHUB_OUTPUT
          echo "SUPABASE_KEY=${{ secrets.PRODUCTION_SUPABASE_ANON_KEY }}" >> $GITHUB_OUTPUT
          echo "JWT_SECRET=${{ secrets.PRODUCTION_JWT_SECRET }}" >> $GITHUB_OUTPUT
          echo "FRONTEND_URL=${{ secrets.PRODUCTION_FRONTEND_URL }}" >> $GITHUB_OUTPUT
          echo "PROD_DATABASE_POOLER_URL=${{ secrets.PRODUCTION_DATABASE_POOLER_URL }}" >> $GITHUB_OUTPUT
          echo "üöÄ Using Production Database"
        elif [[ "${{ github.ref }}" == "refs/heads/develop" ]]; then
          # Develop ‰ΩøÁî®Áç®Á´ãË≥áÊñôÂ∫´
          echo "DATABASE_URL=${{ secrets.DEVELOP_DATABASE_URL }}" >> $GITHUB_OUTPUT
          echo "ALEMBIC_DATABASE_URL=${{ secrets.DEVELOP_DATABASE_POOLER_URL }}" >> $GITHUB_OUTPUT
          echo "SUPABASE_URL=${{ secrets.DEVELOP_SUPABASE_URL }}" >> $GITHUB_OUTPUT
          echo "SUPABASE_KEY=${{ secrets.DEVELOP_SUPABASE_ANON_KEY }}" >> $GITHUB_OUTPUT
          echo "JWT_SECRET=${{ secrets.DEVELOP_JWT_SECRET }}" >> $GITHUB_OUTPUT
          echo "FRONTEND_URL=${{ secrets.DEVELOP_FRONTEND_URL }}" >> $GITHUB_OUTPUT
          echo "PROD_DATABASE_POOLER_URL=${{ secrets.PRODUCTION_DATABASE_POOLER_URL }}" >> $GITHUB_OUTPUT
          echo "üîß Using Develop Database (independent)"
        else
          echo "DATABASE_URL=${{ secrets.STAGING_DATABASE_URL }}" >> $GITHUB_OUTPUT
          echo "ALEMBIC_DATABASE_URL=${{ secrets.STAGING_DATABASE_POOLER_URL }}" >> $GITHUB_OUTPUT
          echo "SUPABASE_URL=${{ secrets.STAGING_SUPABASE_URL }}" >> $GITHUB_OUTPUT
          echo "SUPABASE_KEY=${{ secrets.STAGING_SUPABASE_ANON_KEY }}" >> $GITHUB_OUTPUT
          echo "JWT_SECRET=${{ secrets.STAGING_JWT_SECRET }}" >> $GITHUB_OUTPUT
          echo "FRONTEND_URL=${{ secrets.STAGING_FRONTEND_URL }}" >> $GITHUB_OUTPUT
          echo "PROD_DATABASE_POOLER_URL=${{ secrets.PRODUCTION_DATABASE_POOLER_URL }}" >> $GITHUB_OUTPUT
          echo "üß™ Using Staging Database"
        fi

    - name: Build and push backend image
      run: |
        cd backend && docker buildx build \
          --cache-from type=local,src=/tmp/.buildx-cache \
          --cache-to type=local,dest=/tmp/.buildx-cache-new,mode=max \
          -f Dockerfile \
          -t $REGION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/${{ steps.env_vars.outputs.BACKEND_SERVICE }}:$GITHUB_SHA \
          --push .
        # Ê∏ÖÁêÜÊïèÊÑüÊ™îÊ°à
        rm -f service-account-key.json
        echo "üßπ Cleaned up service account key file"
        # Move cache to prevent it from growing indefinitely
        rm -rf /tmp/.buildx-cache
        mv /tmp/.buildx-cache-new /tmp/.buildx-cache || true

    - name: Setup Python for migrations
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Run Alembic database migrations
      env:
        DATABASE_URL: ${{ steps.db_env.outputs.ALEMBIC_DATABASE_URL }}
      run: |
        echo "üîç Installing dependencies for migrations..."
        pip install -r backend/requirements.txt

        echo "üîÑ Running Alembic database migrations..."
        cd backend
        echo "üìä Current migration status:"
        alembic current || true

        echo "üîÑ Upgrading to latest migration..."
        if ! alembic upgrade head 2>&1 | tee /tmp/alembic_output.txt; then
          # Check if error is due to orphaned revision
          if grep -q "Can't locate revision" /tmp/alembic_output.txt; then
            echo "‚ö†Ô∏è Orphaned revision detected in database"
            echo "üîß Stamping database to latest head to recover..."
            # Get the current head from the codebase
            CURRENT_HEAD=$(alembic heads | head -1 | awk '{print $1}')
            echo "üìå Stamping to revision: $CURRENT_HEAD"
            alembic stamp "$CURRENT_HEAD"
            echo "‚úÖ Database stamped successfully"
          else
            echo "‚ùå Migration failed with unexpected error"
            cat /tmp/alembic_output.txt
            exit 1
          fi
        fi
        echo "‚úÖ Migrations completed"

    - name: üîí Verify RLS Configuration
      env:
        DATABASE_URL: ${{ steps.db_env.outputs.ALEMBIC_DATABASE_URL }}
      run: |
        echo "üîç Checking RLS status for all tables..."

        # Ê™¢Êü•Ê≤íÊúâÂïüÁî® RLS ÁöÑË°®
        # ÊéíÈô§ÊâÄÊúâÊ•≠ÂãôË°®ÔºöÁ≥ªÁµ±‰ΩøÁî® JWT authÔºà‰∏çÊòØ Supabase AuthÔºâÔºåauth.uid() Ê∞∏ÈÅ†ÊòØ NULL
        TABLES_WITHOUT_RLS=$(psql "$DATABASE_URL" -t -c "
          SELECT tablename
          FROM pg_tables
          WHERE schemaname = 'public'
            AND NOT rowsecurity
            AND tablename != 'alembic_version'
            AND tablename NOT IN (
              -- Duotopia Ê•≠ÂãôË°®Ôºà‰ΩøÁî® JWT authÔºåÈùû Supabase AuthÔºâ
              'teachers', 'students', 'classrooms', 'classroom_students',
              'programs', 'lessons', 'content', 'content_item', 'contents', 'content_items',
              'assignments', 'assignment_content', 'assignment_contents', 'student_assignments',
              'student_content_progress', 'student_item_progress',
              'subscription_periods', 'point_usage_logs',
              'teacher_subscription_transaction', 'teacher_subscription_transactions', 'invoice_status_history',
              'user_word_progress', 'practice_sessions', 'practice_answers',
              'organizations', 'schools', 'teacher_organizations', 'teacher_schools', 'classroom_schools', 'student_schools',
              'organization_points_log',
              'program_copy_logs',
              'demo_config',
              -- n8n Á≥ªÁµ±Ë°®ÔºàÂ§ñÈÉ®ÊúçÂãôÔºå‰∏çÈÅ©Áî® RLSÔºâ
              'annotation_tag_entity', 'auth_identity', 'auth_provider_sync_history', 'binary_data',
              'chat_hub_agents', 'chat_hub_messages', 'chat_hub_sessions', 'credentials_entity',
              'data_table', 'data_table_column', 'dynamic_credential_entry', 'dynamic_credential_resolver',
              'event_destinations', 'execution_annotation_tags', 'execution_annotations', 'execution_data',
              'execution_entity', 'execution_metadata', 'folder', 'folder_tag',
              'insights_by_period', 'insights_metadata', 'insights_raw', 'installed_nodes', 'installed_packages',
              'invalid_auth_token', 'migrations', 'oauth_access_tokens', 'oauth_authorization_codes',
              'oauth_clients', 'oauth_refresh_tokens', 'oauth_user_consents', 'processed_data',
              'project', 'project_relation', 'role', 'role_scope', 'scope', 'settings',
              'shared_credentials', 'shared_workflow', 'tag_entity', 'test_case_execution', 'test_run',
              'user', 'user_api_keys', 'variables', 'webhook_entity',
              'workflow_dependency', 'workflow_entity', 'workflow_history', 'workflow_publish_history',
              'workflow_statistics', 'workflows_tags'
            )
          ORDER BY tablename;
        " | xargs)

        if [ ! -z "$TABLES_WITHOUT_RLS" ]; then
          echo "‚ùå ERROR: The following tables do not have RLS enabled:"
          for table in $TABLES_WITHOUT_RLS; do
            echo "   - $table"
          done
          echo ""
          echo "üìù Security Issue: All tables must have Row Level Security enabled!"
          echo "Please add RLS to your migration or run:"
          echo "   psql \$DATABASE_URL -f backend/migrations/enable_rls_all_tables.sql"
          exit 1
        fi

        # Ê™¢Êü•ÂïüÁî®‰∫Ü RLS ‰ΩÜÊ≤íÊúâ Policy ÁöÑË°®
        TABLES_WITHOUT_POLICIES=$(psql "$DATABASE_URL" -t -c "
          SELECT t.tablename
          FROM pg_tables t
          WHERE t.schemaname = 'public'
            AND t.rowsecurity = true
            AND NOT EXISTS (
              SELECT 1 FROM pg_policies p
              WHERE p.tablename = t.tablename
            )
          ORDER BY t.tablename;
        " | xargs)

        if [ ! -z "$TABLES_WITHOUT_POLICIES" ]; then
          echo "‚ö†Ô∏è  WARNING: The following tables have RLS enabled but no policies:"
          for table in $TABLES_WITHOUT_POLICIES; do
            echo "   - $table"
          done
          echo ""
          echo "Note: Tables without policies will deny all access."
          echo "This may be intentional, but please verify."
        fi

        echo "‚úÖ All tables have RLS enabled"

    - name: Deploy backend to Cloud Run
      run: |
        # Áí∞Â¢ÉËÆäÊï∏Ë®≠ÂÆö
        ENV_VARS="DATABASE_URL=${{ steps.db_env.outputs.DATABASE_URL }}"
        ENV_VARS="$ENV_VARS,DATABASE_POOLER_URL=${{ steps.db_env.outputs.ALEMBIC_DATABASE_URL }}"
        ENV_VARS="$ENV_VARS,DATABASE_TYPE=supabase"
        # Audio error monitoring always connects to production Supabase
        ENV_VARS="$ENV_VARS,PROD_DATABASE_POOLER_URL=${{ steps.db_env.outputs.PROD_DATABASE_POOLER_URL }}"
        ENV_VARS="$ENV_VARS,JWT_SECRET=${{ steps.db_env.outputs.JWT_SECRET }}"
        ENV_VARS="$ENV_VARS,JWT_ALGORITHM=HS256"
        ENV_VARS="$ENV_VARS,JWT_EXPIRE_MINUTES=1440"
        ENV_VARS="$ENV_VARS,ENVIRONMENT=${{ steps.env_vars.outputs.ENV_NAME }}"
        ENV_VARS="$ENV_VARS,SUPABASE_URL=${{ steps.db_env.outputs.SUPABASE_URL }}"
        ENV_VARS="$ENV_VARS,SUPABASE_KEY=${{ steps.db_env.outputs.SUPABASE_KEY }}"
        ENV_VARS="$ENV_VARS,OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}"
        ENV_VARS="$ENV_VARS,USE_VERTEX_AI=${{ vars.USE_VERTEX_AI || 'false' }}"
        ENV_VARS="$ENV_VARS,VERTEX_AI_PROJECT_ID=$PROJECT_ID"
        ENV_VARS="$ENV_VARS,VERTEX_AI_LOCATION=us-central1"
        ENV_VARS="$ENV_VARS,SMTP_HOST=${{ secrets.SMTP_HOST }}"
        ENV_VARS="$ENV_VARS,SMTP_PORT=${{ secrets.SMTP_PORT }}"
        ENV_VARS="$ENV_VARS,SMTP_USER=${{ secrets.SMTP_USER }}"
        ENV_VARS="$ENV_VARS,SMTP_PASSWORD=${{ secrets.SMTP_PASSWORD }}"
        ENV_VARS="$ENV_VARS,FROM_EMAIL=${{ secrets.FROM_EMAIL }}"
        ENV_VARS="$ENV_VARS,FROM_NAME=${{ secrets.FROM_NAME }}"
        ENV_VARS="$ENV_VARS,FRONTEND_URL=${{ steps.db_env.outputs.FRONTEND_URL }}"
        ENV_VARS="$ENV_VARS,AZURE_SPEECH_KEY=${{ secrets.AZURE_SPEECH_KEY }}"
        ENV_VARS="$ENV_VARS,AZURE_SPEECH_REGION=${{ secrets.AZURE_SPEECH_REGION }}"
        ENV_VARS="$ENV_VARS,AZURE_SPEECH_ENDPOINT=${{ secrets.AZURE_SPEECH_ENDPOINT }}"
        ENV_VARS="$ENV_VARS,GCP_PROJECT_ID=$PROJECT_ID"
        ENV_VARS="$ENV_VARS,REGION=$REGION"
        ENV_VARS="$ENV_VARS,GCS_BUCKET_NAME=duotopia-audio"
        ENV_VARS="$ENV_VARS,USE_GCS_STORAGE=true"

        # TapPay ÈÖçÁΩÆÔºàÁí∞Â¢ÉÁõ∏ÈóúÔºâ
        ENV_VARS="$ENV_VARS,TAPPAY_SANDBOX_APP_ID=${{ secrets.TAPPAY_SANDBOX_APP_ID }}"
        ENV_VARS="$ENV_VARS,TAPPAY_SANDBOX_APP_KEY=${{ secrets.TAPPAY_SANDBOX_APP_KEY }}"
        ENV_VARS="$ENV_VARS,TAPPAY_SANDBOX_PARTNER_KEY=${{ secrets.TAPPAY_SANDBOX_PARTNER_KEY }}"
        ENV_VARS="$ENV_VARS,TAPPAY_SANDBOX_MERCHANT_ID=${{ secrets.TAPPAY_SANDBOX_MERCHANT_ID }}"
        ENV_VARS="$ENV_VARS,TAPPAY_PRODUCTION_APP_ID=${{ secrets.TAPPAY_PRODUCTION_APP_ID }}"
        ENV_VARS="$ENV_VARS,TAPPAY_PRODUCTION_APP_KEY=${{ secrets.TAPPAY_PRODUCTION_APP_KEY }}"
        ENV_VARS="$ENV_VARS,TAPPAY_PRODUCTION_PARTNER_KEY=${{ secrets.TAPPAY_PRODUCTION_PARTNER_KEY }}"
        ENV_VARS="$ENV_VARS,TAPPAY_PRODUCTION_MERCHANT_ID=${{ secrets.TAPPAY_PRODUCTION_MERCHANT_ID }}"

        # Environment-specific configuration
        if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
          ENV_VARS="$ENV_VARS,TAPPAY_ENV=production"
          ENV_VARS="$ENV_VARS,USE_MOCK_PAYMENT=false"
          ENV_VARS="$ENV_VARS,CRON_SECRET=${{ secrets.PRODUCTION_CRON_SECRET }}"
          ENV_VARS="$ENV_VARS,ENABLE_PAYMENT=${{ secrets.PRODUCTION_ENABLE_PAYMENT }}"
          ENV_VARS="$ENV_VARS,DEMO_ALLOWED_ORIGINS=https://duotopia.co,https://www.duotopia.net"

          # üí∞ Production: Ë≥áÊ∫êÈÖçÁΩÆ
          MIN_INSTANCES=0      # Scale-to-Zero ÁØÄÁúÅÊàêÊú¨
          MEMORY="512Mi"       # Production ÈúÄË¶ÅÊõ¥Â§öË®òÊÜ∂È´î
          CPU="1000m"          # 1 CPU
          CONCURRENCY=80       # È´ò‰ΩµÁôº
          MAX_INSTANCES=6      # ÊîØÊè¥Êõ¥È´òÊµÅÈáè
        elif [[ "${{ github.ref }}" == "refs/heads/develop" ]]; then
          ENV_VARS="$ENV_VARS,TAPPAY_ENV=sandbox"
          ENV_VARS="$ENV_VARS,USE_MOCK_PAYMENT=false"
          ENV_VARS="$ENV_VARS,CRON_SECRET=${{ secrets.DEVELOP_CRON_SECRET }}"
          ENV_VARS="$ENV_VARS,ENABLE_PAYMENT=${{ secrets.DEVELOP_ENABLE_PAYMENT }}"
          ENV_VARS="$ENV_VARS,DEMO_ALLOWED_ORIGINS=${{ secrets.DEVELOP_FRONTEND_URL }}"

          # üí∞ Develop: ÊúÄÂ∞èÂåñÊàêÊú¨
          MIN_INSTANCES=0      # Scale-to-Zero ÊúÄÂ§ßÂåñÁØÄÁúÅÊàêÊú¨
          MEMORY="256Mi"
          CPU="500m"           # 0.5 CPU
          CONCURRENCY=1        # CPU < 1 ÊôÇÂøÖÈ†àË®≠ÁÇ∫ 1
          MAX_INSTANCES=1      # Develop ÊµÅÈáè‰ΩéÔºåÈôçÁÇ∫ 1 ÁØÄÁúÅÊàêÊú¨
        else
          ENV_VARS="$ENV_VARS,TAPPAY_ENV=production"  # Staging uses production TapPay
          ENV_VARS="$ENV_VARS,USE_MOCK_PAYMENT=false"
          ENV_VARS="$ENV_VARS,CRON_SECRET=${{ secrets.STAGING_CRON_SECRET }}"
          ENV_VARS="$ENV_VARS,ENABLE_PAYMENT=${{ secrets.STAGING_ENABLE_PAYMENT }}"
          ENV_VARS="$ENV_VARS,DEMO_ALLOWED_ORIGINS=${{ secrets.STAGING_FRONTEND_URL }}"

          # üí∞ Staging: ÊúÄÂ∞èÂåñÊàêÊú¨
          MIN_INSTANCES=0      # Scale-to-Zero ÊúÄÂ§ßÂåñÁØÄÁúÅÊàêÊú¨
          MEMORY="256Mi"
          CPU="500m"           # 0.5 CPU
          CONCURRENCY=1        # CPU < 1 ÊôÇÂøÖÈ†àË®≠ÁÇ∫ 1
          MAX_INSTANCES=1      # Staging ÊµÅÈáè‰ΩéÔºåÈôçÁÇ∫ 1 ÁØÄÁúÅÊàêÊú¨
        fi

        # Use ~ as custom delimiter instead of , because env var values contain
        # commas (DEMO_ALLOWED_ORIGINS) and @ (DATABASE_URL postgresql://...@host)
        # sed replaces only commas followed by KEY= pattern, preserving commas inside values
        ENV_VARS_ESCAPED=$(echo "$ENV_VARS" | sed 's/,\([A-Z_][A-Z_0-9]*=\)/~\1/g')

        gcloud run deploy ${{ steps.env_vars.outputs.BACKEND_SERVICE }} \
          --image $REGION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/${{ steps.env_vars.outputs.BACKEND_SERVICE }}:$GITHUB_SHA \
          --platform managed \
          --region $REGION \
          --allow-unauthenticated \
          --port 8080 \
          --memory $MEMORY \
          --cpu $CPU \
          --max-instances $MAX_INSTANCES \
          --min-instances $MIN_INSTANCES \
          --cpu-throttling \
          --concurrency $CONCURRENCY \
          --no-cpu-boost \
          --set-env-vars="^~^$ENV_VARS_ESCAPED"

    - name: üßπ Cleanup Old Backend Images
      run: |
        echo "üîç Cleaning up old backend container images..."
        KEEP_COUNT=2
        BACKEND_IMAGES=$(gcloud artifacts docker images list \
          $REGION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/${{ steps.env_vars.outputs.BACKEND_SERVICE }} \
          --format="value(version)" \
          --sort-by="~UPDATE_TIME" 2>/dev/null || echo "")

        if [ ! -z "$BACKEND_IMAGES" ]; then
          TOTAL=$(echo "$BACKEND_IMAGES" | wc -l | xargs)
          echo "  Found $TOTAL backend images (keeping $KEEP_COUNT)"
          if [ $TOTAL -gt $KEEP_COUNT ]; then
            TO_DELETE=$(echo "$BACKEND_IMAGES" | tail -n +$((KEEP_COUNT + 1)))
            DELETE_COUNT=$(echo "$TO_DELETE" | wc -l | xargs)
            echo "  Deleting $DELETE_COUNT old backend images..."
            echo "$TO_DELETE" | while read DIGEST; do
              echo "    Deleting: $DIGEST"
              gcloud artifacts docker images delete \
                "$REGION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/${{ steps.env_vars.outputs.BACKEND_SERVICE }}@${DIGEST}" \
                --quiet --delete-tags 2>/dev/null || echo "    Failed to delete (may be in use)"
            done
          fi
        fi

    - name: Health check
      run: |
        BACKEND_URL=$(gcloud run services describe ${{ steps.env_vars.outputs.BACKEND_SERVICE }} --platform managed --region $REGION --format 'value(status.url)')
        echo "üîç Checking backend health at $BACKEND_URL/api/health"

        for i in {1..5}; do
          if curl -f "$BACKEND_URL/api/health" --max-time 10; then
            echo "‚úÖ Backend is healthy!"
            break
          else
            echo "‚è≥ Waiting for backend to be ready (attempt $i/5)..."
            sleep 10
          fi
        done

    - name: üîç Deployment Verification
      run: |
        echo "üîç Verifying deployment success..."

        # 1Ô∏è‚É£ Cloud Run ÈÉ®ÁΩ≤Á¢∫Ë™ç
        echo "üì¶ Checking Cloud Run revision..."
        LATEST_REVISION=$(gcloud run revisions list \
          --service ${{ steps.env_vars.outputs.BACKEND_SERVICE }} \
          --platform managed \
          --region $REGION \
          --limit 1 \
          --format 'value(metadata.name)')

        CREATION_TIME=$(gcloud run revisions describe $LATEST_REVISION \
          --platform managed \
          --region $REGION \
          --format 'value(metadata.creationTimestamp)')

        echo "‚úÖ Latest revision: $LATEST_REVISION"
        echo "‚úÖ Created at: $CREATION_TIME"

        # 2Ô∏è‚É£ ÊúçÂãôÂÅ•Â∫∑Ê™¢Êü•
        echo "ü©∫ Checking service health..."
        BACKEND_URL=$(gcloud run services describe ${{ steps.env_vars.outputs.BACKEND_SERVICE }} \
          --platform managed \
          --region $REGION \
          --format 'value(status.url)')

        HEALTH_RESPONSE=$(curl -s "$BACKEND_URL/health" || echo "ERROR")
        if echo "$HEALTH_RESPONSE" | grep -q "healthy"; then
          echo "‚úÖ Health check passed"
          echo "$HEALTH_RESPONSE" | jq '.' || echo "$HEALTH_RESPONSE"
        else
          echo "‚ùå Health check failed: $HEALTH_RESPONSE"
          exit 1
        fi

        # 3Ô∏è‚É£ Áí∞Â¢ÉËÆäÊï∏È©óË≠â
        echo "üîß Verifying environment configuration..."
        if echo "$HEALTH_RESPONSE" | grep -q '"environment": "${{ steps.env_vars.outputs.ENV_NAME }}"'; then
          echo "‚úÖ Environment correctly set to: ${{ steps.env_vars.outputs.ENV_NAME }}"
        else
          echo "‚ö†Ô∏è Environment configuration may not match"
        fi

    - name: Deployment Summary
      run: |
        echo "üéâ Backend deployment completed and verified!"
        echo "üì¶ Environment: ${{ steps.env_vars.outputs.ENV_NAME }}"
        echo "üè∑Ô∏è Version: ${{ github.sha }}"
        BACKEND_URL=$(gcloud run services describe ${{ steps.env_vars.outputs.BACKEND_SERVICE }} --platform managed --region $REGION --format 'value(status.url)')
        echo "üåê Backend URL: $BACKEND_URL"
