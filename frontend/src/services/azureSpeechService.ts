import * as sdk from "microsoft-cognitiveservices-speech-sdk";
import axios from "axios";

interface TokenCache {
  token: string;
  region: string;
  expiresAt: Date;
}

export class AzureSpeechService {
  private tokenCache: TokenCache | null = null;

  /**
   * è·å– Azure Speech Tokenï¼ˆå¸¦ç¼“å­˜ï¼Œæå‰1åˆ†é’Ÿè¿‡æœŸï¼‰
   * @private
   */
  private async getToken(): Promise<{ token: string; region: string }> {
    console.log("ğŸ”‘ [TOKEN] æ£€æŸ¥ cache...", {
      hasCachedToken: !!this.tokenCache,
      cacheExpired: this.tokenCache
        ? new Date() >= this.tokenCache.expiresAt
        : "no cache",
    });

    // æ£€æŸ¥ cache æ˜¯å¦æœ‰æ•ˆï¼ˆæå‰1åˆ†é’Ÿè¿‡æœŸï¼‰
    if (this.tokenCache && new Date() < this.tokenCache.expiresAt) {
      console.log("âœ… [TOKEN] ä½¿ç”¨ç¼“å­˜çš„ token");
      return {
        token: this.tokenCache.token,
        region: this.tokenCache.region,
      };
    }

    console.log("ğŸ”‘ [TOKEN] ç¼“å­˜è¿‡æœŸæˆ–ä¸å­˜åœ¨ï¼Œè¯·æ±‚æ–° token...");

    // Cache è¿‡æœŸæˆ–ä¸å­˜åœ¨ï¼Œé‡æ–°è·å–
    try {
      const response = await axios.post("/api/azure-speech/token");
      const { token, region, expires_in } = response.data;

      console.log("âœ… [TOKEN] æ–° token è·å–æˆåŠŸ", {
        region,
        expires_in,
        tokenLength: token.length,
      });

      // Cache tokenï¼ˆæå‰1åˆ†é’Ÿè¿‡æœŸ = 9åˆ†é’Ÿæœ‰æ•ˆï¼‰
      this.tokenCache = {
        token,
        region,
        expiresAt: new Date(Date.now() + (expires_in - 60) * 1000),
      };

      return { token, region };
    } catch (error) {
      console.error("âŒ [TOKEN] è·å–å¤±è´¥", {
        error: (error as { response?: { status?: number } }).response?.status,
        message: (error as Error).message,
      });
      throw new Error("æ— æ³•è·å–è¯­éŸ³åˆ†ææˆæƒï¼Œè¯·åˆ·æ–°é¡µé¢é‡è¯•");
    }
  }

  /**
   * ç›´æ¥è°ƒç”¨ Azure Speech API åˆ†æå‘éŸ³
   *
   * @param audioBlob å½•éŸ³ Blob (WAV æ ¼å¼)
   * @param referenceText å‚è€ƒæ–‡æœ¬
   * @param retryCount å†…éƒ¨é‡è¯•è®¡æ•°ï¼ˆç”¨æˆ·æ— éœ€ä¼ å…¥ï¼‰
   * @returns åˆ†æç»“æœå’Œå»¶è¿Ÿæ—¶é—´
   */
  async analyzePronunciation(
    audioBlob: Blob,
    referenceText: string,
    retryCount = 0,
  ): Promise<{
    result: sdk.PronunciationAssessmentResult;
    latencyMs: number;
  }> {
    console.log("ğŸ”µ [AZURE] analyzePronunciation å¼€å§‹", {
      audioBlobSize: audioBlob.size,
      referenceText: referenceText.substring(0, 50) + "...",
      retryCount,
    });

    const startTime = Date.now();

    try {
      // 1. è·å–çŸ­æ•ˆ token
      console.log("ğŸ”‘ [AZURE] è·å– token...");
      const { token, region } = await this.getToken();
      console.log("ğŸ”‘ [AZURE] Token è·å–æˆåŠŸ", {
        region,
        tokenLength: token.length,
      });

      // 2. é…ç½® Speech SDK
      console.log("âš™ï¸ [AZURE] é…ç½® Speech SDK...");
      const speechConfig = sdk.SpeechConfig.fromAuthorizationToken(
        token,
        region,
      );
      speechConfig.speechRecognitionLanguage = "en-US";

      // 3. é…ç½®å‘éŸ³è¯„ä¼°å‚æ•°
      const pronunciationConfig = new sdk.PronunciationAssessmentConfig(
        referenceText,
        sdk.PronunciationAssessmentGradingSystem.HundredMark,
        sdk.PronunciationAssessmentGranularity.Phoneme,
        true, // enableMiscue
      );

      // 4. ä» Blob åˆ›å»ºéŸ³é¢‘é…ç½®
      const audioFile = new File([audioBlob], "recording.wav", {
        type: "audio/wav",
      });
      const audioConfig = sdk.AudioConfig.fromWavFileInput(audioFile);

      // 5. åˆ›å»ºè¯­éŸ³è¯†åˆ«å™¨
      const recognizer = new sdk.SpeechRecognizer(speechConfig, audioConfig);
      pronunciationConfig.applyTo(recognizer);

      console.log("ğŸ™ï¸ [AZURE] å¼€å§‹è¯†åˆ«...");

      // 6. æ‰§è¡Œè¯†åˆ«ï¼ˆPromise åŒ…è£…ï¼‰
      return new Promise((resolve, reject) => {
        recognizer.recognizeOnceAsync(
          (result) => {
            const latencyMs = Date.now() - startTime;

            // è¯†åˆ«æˆåŠŸ
            if (result.reason === sdk.ResultReason.RecognizedSpeech) {
              console.log("âœ… [AZURE] è¯†åˆ«æˆåŠŸ", {
                latencyMs: `${latencyMs}ms`,
                reason: result.reason,
              });

              const pronunciationResult =
                sdk.PronunciationAssessmentResult.fromResult(result);

              recognizer.close();
              resolve({ result: pronunciationResult, latencyMs });
            }
            // Token å¯èƒ½è¿‡æœŸ - è‡ªåŠ¨ retry
            else if (
              (result.reason === sdk.ResultReason.NoMatch ||
                result.errorDetails?.includes("401")) &&
              retryCount === 0
            ) {
              console.warn("âš ï¸ [AZURE] Token å¯èƒ½è¿‡æœŸï¼Œretry...", {
                reason: result.reason,
                errorDetails: result.errorDetails,
              });
              this.tokenCache = null; // æ¸…é™¤æ—§ token
              recognizer.close();

              // é€’å½’é‡è¯•ï¼ˆåªé‡è¯•ä¸€æ¬¡ï¼‰
              resolve(this.analyzePronunciation(audioBlob, referenceText, 1));
            }
            // å…¶ä»–é”™è¯¯
            else {
              console.error("âŒ [AZURE] è¯†åˆ«å¤±è´¥", {
                reason: result.reason,
                errorDetails: result.errorDetails,
              });

              recognizer.close();
              const errorMsg =
                result.errorDetails || `è¯†åˆ«å¤±è´¥: ${result.reason}`;
              reject(new Error(errorMsg));
            }
          },
          (error) => {
            console.error("âŒ [AZURE] è¯†åˆ«é”™è¯¯", { error });

            recognizer.close();

            // 401 é”™è¯¯ - è‡ªåŠ¨ retry
            if (error.includes("401") && retryCount === 0) {
              console.warn("âš ï¸ [AZURE] 401 é”™è¯¯ï¼Œretry...");
              this.tokenCache = null;
              resolve(this.analyzePronunciation(audioBlob, referenceText, 1));
            } else {
              reject(new Error(`åˆ†æå¤±è´¥: ${error}`));
            }
          },
        );
      });
    } catch (error) {
      console.error("âŒ [AZURE] analyzePronunciation å¼‚å¸¸", error);
      throw new Error("è¯­éŸ³åˆ†æå¤±è´¥ï¼Œè¯·é‡è¯•");
    }
  }

  /**
   * èƒŒæ™¯ä¸Šä¼ éŸ³æ¡£å’Œåˆ†æç»“æœï¼ˆä¸é˜»å¡ UIï¼‰
   *
   * @param audioBlob å½•éŸ³ Blob
   * @param analysisResult åˆ†æç»“æœ
   * @param latencyMs å®¢æˆ·ç«¯åˆ° Azure çš„å»¶è¿Ÿ
   */
  async uploadAnalysisInBackground(
    audioBlob: Blob,
    analysisResult: sdk.PronunciationAssessmentResult | Record<string, unknown>,
    latencyMs: number,
  ): Promise<void> {
    try {
      const formData = new FormData();
      formData.append("audio_file", audioBlob, "recording.wav");
      formData.append("analysis_json", JSON.stringify(analysisResult));
      formData.append("latency_ms", latencyMs.toString());

      // èƒŒæ™¯ä¸Šä¼ ï¼Œä¸ç­‰å¾…ç»“æœï¼ˆcatch æ•è·é”™è¯¯ä½†ä¸æŠ›å‡ºï¼‰
      axios
        .post("/api/speech/upload-analysis", formData, {
          headers: { "Content-Type": "multipart/form-data" },
        })
        .catch((error) => {
          console.error("Background upload failed:", error);
          // å¯é€‰ï¼šå­˜åˆ° localStorage å¾…åç»­é‡è¯•
          // this.saveFailedUpload({ audioBlob, analysisResult, latencyMs });
        });
    } catch (error) {
      console.error("Failed to prepare background upload:", error);
    }
  }
}

// Singleton instance
export const azureSpeechService = new AzureSpeechService();
